{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abbc372",
   "metadata": {},
   "source": [
    "# CS230 Fall 2024 Project Proposal: Transformer-based Surrogate Model for Subsurface Flow Simulation (Time Series Forecasting)\n",
    "### Guido Di Federico, 0066465222\n",
    "### Department of Energy Science and Engineering, Stanford University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfbc791",
   "metadata": {},
   "source": [
    "### Load modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import shutil\n",
    "import tempfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import time as timeModule\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import L1Loss\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss, MAPE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d805d",
   "metadata": {},
   "source": [
    "### User functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_dataframe(num_samples, params, seed_value=0):\n",
    "    \"\"\"\n",
    "    Create a random DataFrame with specified ranges for different parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_samples: int\n",
    "        The number of samples to generate.\n",
    "\n",
    "    params: dict\n",
    "        A dictionary with keys 'bhp_inj', 'k1', and 'f1', and value pairs \n",
    "        representing ranges for each parameter.\n",
    "\n",
    "    seed_value: int, optional\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: pd.DataFrame\n",
    "        A DataFrame with randomly generated columns 'bhp_inj', 'k1', 'f1', 'k2', \n",
    "        'k3', 'f2', and 'f3' based on specified ranges and calculations.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    data = {\n",
    "        'bhp_inj': np.round(np.random.uniform(params['bhp_inj'][0], params['bhp_inj'][1], num_samples)).astype(int),\n",
    "        'k1': np.random.uniform(params['k1'][0], params['k1'][1], num_samples),\n",
    "        'f1': np.random.uniform(params['f1'][0], params['f1'][1], num_samples),\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df['k2'] = df['k1'] * 10.0\n",
    "    df['k3'] = df['k1'] * 50.0\n",
    "    df['f2'] = df['f1'] * 1.5\n",
    "    df['f3'] = df['f1'] * 2.0\n",
    "    \n",
    "    df[['k1', 'k2', 'k3']] = df[['k1', 'k2', 'k3']].round(0).astype(int)\n",
    "    df[['f1', 'f2', 'f3']] = df[['f1', 'f2', 'f3']].round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def model2tricat(model, thresh1, thresh2):\n",
    "    \"\"\"\n",
    "    Categorize model values into three categories based on threshold values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: np.ndarray\n",
    "        The model data array to categorize.\n",
    "\n",
    "    thresh1: float\n",
    "        The lower threshold for categorization.\n",
    "\n",
    "    thresh2: float\n",
    "        The upper threshold for categorization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_copy: np.ndarray\n",
    "        The categorized model data, with values 0, 1, or 2 based on thresholds.\n",
    "    \"\"\"\n",
    "    model_copy = np.copy(model)\n",
    "    model_copy[model_copy < thresh1] = 0.\n",
    "    model_copy[(model_copy >= thresh1) & (model_copy <= thresh2)] = 1.\n",
    "    model_copy[model_copy > thresh2] = 2.\n",
    "    \n",
    "    return model_copy\n",
    "\n",
    "\n",
    "def files_from_folder(folder, results_file):\n",
    "    \"\"\"\n",
    "    List all files with the same name across subfolders in a given folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder: str\n",
    "        The main folder containing subfolders with the target files.\n",
    "\n",
    "    results_file: str\n",
    "        The name of the file present in each subfolder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sorted_full_paths: list\n",
    "        List of absolute paths to the specified files in each subfolder.\n",
    "    \"\"\"\n",
    "    files = os.listdir(folder)\n",
    "    sorted_files = sorted([os.path.join(folder, file_name) for file_name in files])\n",
    "    \n",
    "    idxs = [int(file.split('_')[-1]) for file in sorted_files]\n",
    "    \n",
    "    sorted_files_human = [\"\" for idx in range(len(sorted_files))]\n",
    "    \n",
    "    for i, idx in enumerate(idxs):\n",
    "        sorted_files_human[idx] = sorted_files[i]\n",
    "        \n",
    "    sorted_full_paths = [file_name + results_file for file_name in sorted_files_human]\n",
    "\n",
    "    return sorted_full_paths\n",
    "\n",
    "\n",
    "def extract_sim_output(files, times, QoI):\n",
    "    \"\"\"\n",
    "    Extract simulation outputs for specified times and quantities of interest (QoI).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files: list\n",
    "        List of paths to the simulation results files.\n",
    "\n",
    "    times: list\n",
    "        List of times (in days) at which to extract simulation data.\n",
    "\n",
    "    QoI: list\n",
    "        List of strings representing the quantities of interest in the output files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_data: np.ndarray\n",
    "        Extracted data array with shape (#files, #times, #QoI-1).\n",
    "    \"\"\"\n",
    "    all_data = np.zeros((len(files), len(times), len(QoI)-1))\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        path = files[i]\n",
    "        output = pd.read_csv(path, index_col=False, delim_whitespace=True)\n",
    "        \n",
    "        selected_output = output[QoI]\n",
    "        selected_output_times = abs(selected_output[selected_output['Day'].isin(times)].to_numpy()[:,1:])\n",
    "    \n",
    "        all_data[i] = selected_output_times\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "class SaveTrainValLossCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if \"train_loss\" in trainer.callback_metrics:\n",
    "            train_loss = trainer.callback_metrics[\"train_loss\"].item()\n",
    "            self.train_losses.append(train_loss)\n",
    "            print(f\"Epoch {trainer.current_epoch}: train_loss={train_loss}\")\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if \"val_loss\" in trainer.callback_metrics:\n",
    "            val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "            self.val_losses.append(val_loss)\n",
    "            print(f\"Epoch {trainer.current_epoch}: val_loss={val_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1040dbd5",
   "metadata": {},
   "source": [
    "### Set plotting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11126117",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)           \n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     \n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)     \n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    \n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)   \n",
    "plt.rc('legend', fontsize=SMALL_SIZE)     \n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbeab3",
   "metadata": {},
   "source": [
    "### Set additional parameters and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SIZE = 2816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir   = './'\n",
    "pics_dir      = './pics_cs230/'\n",
    "sim_dir       = '/scratch/users/gdifede/adgprs_sims_cs230/'\n",
    "dataset_path  = '/oak/stanford/schools/ees/lou/gdifede/cs230_project/dataset2'\n",
    "\n",
    "if not os.path.exists(pics_dir):\n",
    "    os.makedirs(pics_dir)\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b82c577",
   "metadata": {},
   "source": [
    "### Create dataset of simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24747fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'bhp_inj': (325, 355),\n",
    "    'k1': (30, 50),\n",
    "    'f1': (0.08, 0.12),\n",
    "}\n",
    "\n",
    "df_params_gen = create_random_dataframe(TOTAL_SIZE, params)\n",
    "df_params_gen.to_csv(results_dir + 'params_dataset.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.read_csv(results_dir + 'params_dataset.csv')\n",
    "df_params.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d21b0b",
   "metadata": {},
   "source": [
    "### Launch simulations with ADGPRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slurm_script_path = \"/scratch/users/gdifede/adgprs_sims_cs230/launch.slurm\"\n",
    "#result = subprocess.run([\"sbatch\", slurm_script_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849634bc",
   "metadata": {},
   "source": [
    "## Dataset and features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046941c",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset = Dataset.load_from_disk(dataset_path)\n",
    "models = model2tricat(np.array([np.array(image) for image in loaded_dataset['image']])[:TOTAL_SIZE,:,:], 100, 200)*1. \n",
    "N_sim = models.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_full_paths        = files_from_folder(sim_dir +  'runs_update/', '/states_vars.rates.txt')[:N_sim]\n",
    "\n",
    "dt                       = 50\n",
    "dt_conversion            = 10\n",
    "all_times                = list(np.arange(dt,1300,dt))\n",
    "all_times_new            = list(np.arange(dt,1300,dt) / dt_conversion) \n",
    "end_of_hist_idx = 5\n",
    "all_QoI                  = ['I1:WAT', 'I2:WAT',\n",
    "                           'I3:WAT', 'I4:WAT',\n",
    "                           'P1:WAT','P1:OIL']\n",
    "\n",
    "all_geomodels            = models\n",
    "all_QoI.insert(0,'Day') \n",
    "\n",
    "\n",
    "N_t                      = len(all_times)\n",
    "N_QoI                    = len(all_QoI) - 1\n",
    "dt_new                   = dt / dt_conversion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6617be0e",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c70cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_all_times_petrel = extract_sim_output(sorted_full_paths, all_times, all_QoI)\n",
    "all_QoI = [item.replace('OIL', 'NAPL') for item in all_QoI]\n",
    "\n",
    "all_times_array           = np.array(all_times*N_sim).reshape((N_sim,-1,1))\n",
    "all_labels                = np.arange(N_sim)\n",
    "labels_array              = np.repeat(all_labels, N_t).reshape((N_sim,-1,1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ffd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {}\n",
    "\n",
    "for item in all_QoI:\n",
    "    if 'NAPL' in item:\n",
    "        color_dict[item] = 'green'\n",
    "    elif 'P' in item and 'WAT' in item:\n",
    "        color_dict[item] = 'steelblue'\n",
    "    elif 'I' in item and 'WAT' in item:\n",
    "        color_dict[item] = 'steelblue'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7729d2",
   "metadata": {},
   "source": [
    "### Examples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27bbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = random.sample(range(N_sim), 30)\n",
    "\n",
    "for QoI in all_QoI[1:]:\n",
    "    QoI_idx = all_QoI.index(QoI) - 1\n",
    "    plt.figure(QoI_idx)\n",
    "    plt.axvline(x=dt_new * (end_of_hist_idx + 1), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "\n",
    "    for i in range(len(random_numbers)):\n",
    "        run_idx = random_numbers[i]\n",
    "        plt.plot(all_times_new[:end_of_hist_idx + 1], all_data_all_times_petrel[run_idx, :end_of_hist_idx + 1, QoI_idx], \n",
    "                 color=color_dict[QoI], alpha=0.8)\n",
    "        plt.plot(all_times_new[end_of_hist_idx:], all_data_all_times_petrel[run_idx, end_of_hist_idx:, QoI_idx], \n",
    "                 color=color_dict[QoI], alpha=0.25)\n",
    "\n",
    "    plt.plot(all_times_new[:end_of_hist_idx + 1], all_data_all_times_petrel[0, :end_of_hist_idx + 1, QoI_idx], \n",
    "             color=color_dict[QoI], alpha=0.8, label='Historical')\n",
    "    plt.plot(all_times_new[end_of_hist_idx:], all_data_all_times_petrel[0, end_of_hist_idx:, QoI_idx], \n",
    "             color=color_dict[QoI], alpha=0.25, label='Future')\n",
    "\n",
    "    plt.xlabel('Time (day)')\n",
    "    plt.ylabel('Flow rate (m$^3$/day)')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(QoI)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.gca().yaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "\n",
    "\n",
    "    plt.savefig(pics_dir + f'{QoI}_example.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634393fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = random.sample(range(N_sim), 30)\n",
    "\n",
    "for QoI in all_QoI[1:]:\n",
    "    QoI_idx = all_QoI.index(QoI) - 1\n",
    "    plt.figure(QoI_idx)\n",
    "    #plt.axvline(x=dt_new * (end_of_hist_idx + 1), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "\n",
    "    for i in range(len(random_numbers)):\n",
    "        run_idx = random_numbers[i]\n",
    "        plt.plot(all_times_new[:], all_data_all_times_petrel[run_idx, :, QoI_idx], \n",
    "                 color=color_dict[QoI], alpha=0.99)\n",
    "\n",
    "    plt.plot(all_times_new[:], all_data_all_times_petrel[0, :, QoI_idx], \n",
    "             color=color_dict[QoI], alpha=0.99)\n",
    "\n",
    "    plt.xlabel('Time (day)')\n",
    "    plt.ylabel('Flow rate (m$^3$/day)')\n",
    "    #plt.legend(loc='best')\n",
    "    plt.title(QoI)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #plt.gca().yaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "\n",
    "\n",
    "    plt.savefig(pics_dir + f'{QoI}_example.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = 5\n",
    "cols = 5\n",
    "\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "random_numbers = random.sample(range(N_sim), num_plots)\n",
    "\n",
    "well_locations = {\n",
    "    'I1': (10, 10),\n",
    "    'I2': (32, 10),\n",
    "    'I3': (32, 54),\n",
    "    'I4': (54, 54),\n",
    "    'P1': (32, 32)\n",
    "}\n",
    "rows = (num_plots + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows *3))\n",
    "\n",
    "axes = axes.flatten()\n",
    "random_indices = random.sample(range(models.shape[0]), num_plots)\n",
    "\n",
    "for i, index in enumerate(random_indices):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(models[index], cmap='viridis')\n",
    "    ax.set_xticks([0, 30, 60])\n",
    "\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.set_yticks([0, 30, 60])\n",
    "    else:\n",
    "        ax.set_yticks([])  # Hide y-ticks for other subplots\n",
    "        ax.yaxis.set_visible(False)  # Optionally hide the entire y-axis\n",
    "\n",
    "    for well, (x, y) in well_locations.items():\n",
    "        x_adjusted = max(0, min(x - 5, models.shape[2] - 10))\n",
    "        y_adjusted = max(0, min(y - 5, models.shape[1] - 10))\n",
    "    \n",
    "        txt = ax.text(x_adjusted, y_adjusted, well, color='white', fontsize=16)\n",
    "        txt.set_path_effects([PathEffects.withStroke(linewidth=1, foreground='k')])\n",
    "\n",
    "        ax.scatter(x-1, y-1, color='r', marker='o' if well[0] == 'P' else 'v', s=100, edgecolor = 'k')\n",
    "        \n",
    "\n",
    "norm = plt.Normalize(vmin=0, vmax=2)\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "colors = [cmap(norm(value)) for value in [0, 1, 2]]\n",
    "labels = ['Mud', 'Levee coarse sand', 'Channel fine sand']\n",
    "legend_handles = [plt.Line2D([0], [0], marker='s', color='w', markersize=16, markerfacecolor=color, label=label, markeredgecolor='k') for color, label in zip(colors, labels)]\n",
    "plt.legend(handles=legend_handles, loc='center', frameon=False, bbox_to_anchor=(-1.9,1.3), fontsize=16, ncol=3)  \n",
    "plt.tight_layout()\n",
    "plt.savefig(pics_dir + f'models_seed{seed_value}_example.jpg', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f959e25",
   "metadata": {},
   "source": [
    "## Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_type = 'pca'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a63791",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd453c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoding_type == 'pca':\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    all_geomodels = all_geomodels.reshape(N_sim, -1)  \n",
    "    all_geomodels_standardized = scaler.fit_transform(all_geomodels)\n",
    "\n",
    "    pca = PCA(n_components=81)  \n",
    "\n",
    "\n",
    "    geomodels_pca          = pca.fit_transform(all_geomodels_standardized)\n",
    "    geomodels_pca_flat     = np.repeat(geomodels_pca, N_t).reshape((geomodels_pca.shape[0], N_t, np.prod(geomodels_pca.shape[1:])))\n",
    "    all_geomodels_flat     = geomodels_pca_flat\n",
    "    \n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(cumulative_explained_variance)#, marker='o', markersize = 6)\n",
    "    plt.title('PCA representation of rock type maps', fontsize = 18 )\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cum. explained variance')\n",
    "    plt.grid()\n",
    "    plt.axhline(y=0.80, color='r', linestyle='--')  # Threshold line at 95%\n",
    "    plt.ylim([0,1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pics_dir + f'pca.jpg', dpi = 300)\n",
    "\n",
    "else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b6b51",
   "metadata": {},
   "source": [
    "### Assemble all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcfedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_labels                = [f\"dim_{i}\" for i in range(all_geomodels_flat.shape[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad979518",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_array      = np.concatenate((labels_array, all_times_array, all_data_all_times_petrel, all_geomodels_flat), axis=-1) \n",
    "flat_stacked_array = stacked_array.reshape((stacked_array.shape[0]*stacked_array.shape[1],stacked_array.shape[2]))\n",
    "df_params_array    = df_params.loc[df_params.index.repeat(N_t)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf8ee4",
   "metadata": {},
   "source": [
    "### Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rates = pd.DataFrame(flat_stacked_array)\n",
    "data_rates.columns     = ['m'] + all_QoI + geo_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rates = pd.concat([data_rates, df_params_array], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d6a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "napl_columns = [col for col in data_rates.columns if 'NAPL' in str(col)]\n",
    "\n",
    "data_rates['NAPL'] = data_rates[napl_columns].sum(axis=1)\n",
    "data_rates        = data_rates.drop(columns=[col for col in napl_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e832836",
   "metadata": {},
   "source": [
    "### Refine labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123375bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rates['m']        = 'model_' + data_rates['m'].astype(str).str[:-2]\n",
    "\n",
    "data_rates[\"time_idx\"] = (data_rates[\"Day\"] / dt ).astype(int)\n",
    "data_rates[\"time_idx\"] -= data_rates[\"time_idx\"].min()\n",
    "\n",
    "for i in range(all_geomodels_flat.shape[-1]):\n",
    "    column_name = f'dim_{i}'\n",
    "    data_rates[column_name] = data_rates[column_name].astype(float)\n",
    "\n",
    "data_rates[\"Day\"] = data_rates[\"Day\"] / dt_conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rates.to_csv(results_dir + 'data_rates.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303f225",
   "metadata": {},
   "source": [
    "## Build TimeSeriesDataset in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d187f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 19\n",
    "max_encoder_length    = end_of_hist_idx + 1\n",
    "training_cutoff       = data_rates[\"time_idx\"].max() - max_prediction_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726d4113",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TimeSeriesDataSet(\n",
    "    data_rates[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"NAPL\",\n",
    "    group_ids=['m'],\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],\n",
    "    static_reals= list(df_params.columns) + geo_labels,\n",
    "    time_varying_known_categoricals=[],\n",
    "    variable_groups={}, \n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "         \"I1:WAT\", \"I2:WAT\", \"I3:WAT\", \"I4:WAT\", \"P1:WAT\", \"NAPL\" \n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"m\"], transformation=\"softplus\"\n",
    "    ), \n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc0bdc",
   "metadata": {},
   "source": [
    "### Validation and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae98b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, data_rates, predict=True, stop_randomization=True)\n",
    "batch_size_baseline = TOTAL_SIZE\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=TOTAL_SIZE)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=TOTAL_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a70c300",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a97049",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "SMAPE()(baseline_predictions.output, baseline_predictions.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a492007f",
   "metadata": {},
   "source": [
    "# Construct TFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222186df",
   "metadata": {},
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = training.to_dataloader(train=True, batch_size=32)\n",
    "val_dataloader = validation.to_dataloader(train=True, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-5, patience=100, verbose=True, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  \n",
    "logger = TensorBoardLogger(\"lightning_logs\") \n",
    "save_train_val_loss_callback = SaveTrainValLossCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"cuda\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    #limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback, save_train_val_loss_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    lstm_layers  = 2,\n",
    "    learning_rate=0.001,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.3,\n",
    "    hidden_continuous_size=16,\n",
    "    loss=SMAPE(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    optimizer=\"Adam\",\n",
    "    #reduce_on_plateau_patience=4,\n",
    ") \n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c7fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "start_time = timeModule.time()\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=train_dataloader,\n",
    ")\n",
    "\n",
    "print(f'Training took {(timeModule.time() - start_time) / 60. } mins.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b40fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_loss = np.array(save_train_val_loss_callback.train_losses)\n",
    "all_val_loss = np.array(save_train_val_loss_callback.val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"tft_cs230.pth\"\n",
    "torch.save(tft.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73266df",
   "metadata": {},
   "source": [
    "### Evaluate peformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tft = tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size_baseline)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1844c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeModule.time()\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cuda\"))\n",
    "print(f'Forecast took {timeModule.time() - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ba46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_predictions = predictions.output.detach().cpu().numpy()\n",
    "sim_true = predictions.y[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = random.sample(range(N_sim), 4)\n",
    "\n",
    "for i in range(len(random_numbers)):\n",
    "    plt.figure(i)\n",
    "    run_idx = random_numbers[i]\n",
    "    \n",
    "    bhp_inj = df_params.loc[run_idx, 'bhp_inj']\n",
    "    k1 = df_params.loc[run_idx, 'k1']\n",
    "    f1 = df_params.loc[run_idx, 'f1']\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1])  # Adjust [3, 1] for desired width ratio\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax1.axvline(x=dt_new * (end_of_hist_idx + 1), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "\n",
    "\n",
    "    for QoI in all_QoI[1:]:\n",
    "        QoI_idx = all_QoI.index(QoI) - 1\n",
    "        ax1.axvline(x=dt_new * (end_of_hist_idx + 1), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "        ax1.plot(all_times_new[:end_of_hist_idx + 1], all_data_all_times_petrel[run_idx, :end_of_hist_idx + 1, QoI_idx], \n",
    "                     color=color_dict[QoI], alpha=0.8, label='Sim. hist. (water)')\n",
    "        ax1.plot(all_times_new[end_of_hist_idx:], all_data_all_times_petrel[run_idx, end_of_hist_idx:, QoI_idx], \n",
    "                     color=color_dict[QoI], alpha=0.25, label='Sim. hist. (water)')\n",
    "\n",
    "    if QoI == 'P1:NAPL':\n",
    "        ax1.plot(all_times_new[end_of_hist_idx:], all_data_all_times_petrel[run_idx, end_of_hist_idx:, QoI_idx], \n",
    "                 color=color_dict[QoI], alpha=0.25, label='Sim. fut.(NAPL)')\n",
    "        ax1.plot(all_times_new[:end_of_hist_idx + 1], all_data_all_times_petrel[run_idx,  :end_of_hist_idx + 1, QoI_idx], \n",
    "                 color=color_dict[QoI], alpha=0.8, label='Sim. hist. (NAPL)')\n",
    "    \n",
    "    ax1.scatter(all_times_new[end_of_hist_idx +1 :], tft_predictions[run_idx, :], \n",
    "             color='red', alpha=0.99, label='Surr. fut.(NAPL)')\n",
    "\n",
    "    ax1.scatter(all_times_new[end_of_hist_idx], all_data_all_times_petrel[run_idx, end_of_hist_idx , QoI_idx], \n",
    "             color='red', alpha=0.99, label='Surr. fut.(NAPL)')\n",
    "\n",
    "        \n",
    "    \n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "\n",
    "    unique_labels = []\n",
    "    unique_handles = []\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if label not in unique_labels:\n",
    "            unique_labels.append(label)\n",
    "            unique_handles.append(handle)\n",
    "\n",
    "    ax1.legend(unique_handles, unique_labels, fontsize = 14, loc = 'upper right')\n",
    "\n",
    "    ax1.set_xlabel('Time (day)')\n",
    "    ax1.set_ylabel('Flow rate (m$^3$/day)')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "    ax1.set_title(f'BHP$_{{inj}}$: {bhp_inj} bar, $k_{{low}}$:{k1} mD, $\\\\phi_{{low}}$: {f1:.2f}')\n",
    "    #ax1.set_xticks(np.linspace(min(all_times_new), max(all_times_new), 6))\n",
    "    \n",
    "\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    ax2.imshow(models[run_idx], cmap='viridis', aspect='auto')  # Adjust cmap as necessary\n",
    "    ax2.set_title('Rock type map')\n",
    "\n",
    "    \n",
    "    for well, (x, y) in well_locations.items():\n",
    "        x_adjusted = max(0, min(x - 5, models.shape[2] - 10))\n",
    "        y_adjusted = max(0, min(y - 5, models.shape[1] - 10))\n",
    "\n",
    "        txt = ax2.text(x_adjusted, y_adjusted, well, color='white', fontsize=25)\n",
    "        txt.set_path_effects([PathEffects.withStroke(linewidth=1, foreground='k')])\n",
    "\n",
    "        ax2.scatter(x-1, y-1, color='r', marker='o' if well[0] == 'P' else 'v', s=100, edgecolor = 'k')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #plt.savefig(pics_dir + f'{QoI}_example_{run_idx}.jpg', dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
